import { useEffect, useRef, useState } from "react";
import {
  DrawingUtils,
  NormalizedLandmark,
  PoseLandmarker,
} from "@mediapipe/tasks-vision";
import { usePoseStore, useVideoStore } from "@/store";

interface UseVideoCanvasProps {
  videoRef: React.RefObject<HTMLVideoElement | null>;
}

const drawSkeleton = (
  ctx: CanvasRenderingContext2D,
  landmarks: NormalizedLandmark[]
) => {
  const drawingUtils = new DrawingUtils(ctx);

  drawingUtils.drawConnectors(landmarks, PoseLandmarker.POSE_CONNECTIONS, {
    color: "#00FF00",
    lineWidth: 3,
  });

  drawingUtils.drawLandmarks(landmarks, {
    color: "#FFFFFF",
    radius: 3,
    fillColor: "#FFFFFF",
  });
};

export function useVideoCanvas({ videoRef }: UseVideoCanvasProps) {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const animationRef = useRef<number>(0);
  const lastDetectionTime = useRef<number>(0);

  const workerRef = useRef<Worker | null>(null);
  const [isWorkerInitialized, setIsWorkerInitialized] = useState(false);

  // ğŸ”¥ ì›Œì»¤ê°€ í˜„ì¬ í¬ì¦ˆ ì¶”ì • ì¤‘ì¸ì§€ ì—¬ë¶€ (busy flag)
  const workerBusyRef = useRef<boolean>(false);

  const { video, setVideoData } = usePoseStore();
  const { source, sourceType, isPlaying } = useVideoStore();

  // ì›Œì»¤ì—ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì‹ í•œ ëœë“œë§ˆí¬ë¥¼ ì €ì¥í•˜ì—¬ ë“œë¡œì‰ì— ì‚¬ìš©
  const lastDrawnLandmarks = useRef<NormalizedLandmark[] | null>(null);

  // ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì„¤ì •
  useEffect(() => {
    const videoElement = videoRef.current;
    if (!videoElement) return;

    if (sourceType === "stream" && source) {
      const mediaStream = source as MediaStream;
      if (videoElement.srcObject !== mediaStream) {
        videoElement.srcObject = mediaStream;
        videoElement.src = "";
        videoElement.play().catch(() => {});
      }
    } else if (sourceType === "url" && source) {
      const videoPath = source as string;
      videoElement.srcObject = null;
      if (videoElement.src !== videoPath) {
        videoElement.src = videoPath;
        videoElement.play().catch(() => {});
      }
    } else {
      videoElement.srcObject = null;
      videoElement.src = "";
    }
  }, [videoRef, source, sourceType]);

  // Worker ì´ˆê¸°í™” ë° í†µì‹  ì„¤ì •
  useEffect(() => {
    const worker = new Worker(
      new URL("../workers/pose-worker.js", import.meta.url),
      { type: "module" }
    );
    workerRef.current = worker;

    worker.onmessage = (event) => {
      const {
        type,
        landmarks,
        angles,
        vectorized,
        poseClass,
        fps,
        latency,
      } = event.data;

      if (type === "INITIALIZED") {
        setIsWorkerInitialized(true);
      } else if (type === "RESULT" && landmarks) {
        // ì›Œì»¤ê°€ ì´ í”„ë ˆì„ ì²˜ë¦¬ë¥¼ ëëƒˆìœ¼ë‹ˆ busy í•´ì œ
        workerBusyRef.current = false;

        // ìµœì‹  í¬ì¦ˆ ê²°ê³¼ ì €ì¥ (ë Œë”ë§ ì‹œ ì‚¬ìš©)
        lastDrawnLandmarks.current = landmarks;

        // Storeì— ë¶„ì„ ê²°ê³¼ ì €ì¥
        setVideoData(landmarks, angles, fps, vectorized, poseClass, latency);
      }
    };

    worker.postMessage({ type: "INIT" });

    return () => {
      worker.terminate();
      workerRef.current = null;
    };
  }, [setVideoData]);

  // ë©”ì¸ ìŠ¤ë ˆë“œ detectFrame (ë Œë”ë§ ë° "ì „ì†¡ ì—¬ë¶€ íŒë‹¨" ë‹´ë‹¹)
  const detectFrame = async (now: number) => {
    const videoElement = videoRef.current;
    const canvas = canvasRef.current;
    const ctx = canvas?.getContext("2d");
    const worker = workerRef.current;

    if (
      !videoElement ||
      !canvas ||
      !ctx ||
      !worker ||
      !videoElement.videoWidth ||
      !videoElement.videoHeight ||
      videoElement.readyState !== videoElement.HAVE_ENOUGH_DATA
    ) {
      return;
    }

    // ì„±ëŠ¥ì„ ìœ„í•´ í•´ìƒë„ ì œí•œ
    videoElement.width = 640;
    videoElement.height = 360;

    const FRAME_INTERVAL = 33; // 30 FPS ëª©í‘œ
    const shouldDetect = now - lastDetectionTime.current >= FRAME_INTERVAL;

    // ìº”ë²„ìŠ¤ í¬ê¸° ë™ê¸°í™”
    if (
      canvas.width !== videoElement.videoWidth ||
      canvas.height !== videoElement.videoHeight
    ) {
      canvas.width = videoElement.videoWidth;
      canvas.height = videoElement.videoHeight;
    }

    // 1. ë¹„ë””ì˜¤ í”„ë ˆì„ ë Œë”ë§
    ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);

    // 2. ì´ì „ í”„ë ˆì„ì˜ ê²°ê³¼ ìŠ¤ì¼ˆë ˆí†¤ ë“œë¡œì‰
    if (lastDrawnLandmarks.current) {
      drawSkeleton(ctx, lastDrawnLandmarks.current);
    }

    // 3. ì›Œì»¤ê°€ í•œê°€í•  ë•Œë§Œ ìƒˆ í”„ë ˆì„ ì „ì†¡ (ğŸ”¥ ì¤‘ìš”)
    if (shouldDetect && !workerBusyRef.current) {
      try {
        workerBusyRef.current = true; // ì›Œì»¤ ì ìœ  ì‹œì‘

        // ImageBitmap ìƒì„± (transferableë¡œ ë³µì‚¬ ë¹„ìš© ì¤„ì´ê¸°)
        const imageBitmap = await createImageBitmap(videoElement);

        worker.postMessage(
          {
            type: "DETECT",
            imageBitmap,
            timestamp: now,
            previousAngles: video.previousAngles,
            lastFrameTime: lastDetectionTime.current,
          },
          [imageBitmap] // transfer
        );

        lastDetectionTime.current = now;
      } catch (e) {
        console.error("Worker communication failed:", e);
        workerBusyRef.current = false; // ì—ëŸ¬ ì‹œì—ë„ busy í•´ì œ
      }
    }
  };

  // ê°ì§€ ë£¨í”„ ì‹œì‘/ì¤‘ì§€
  useEffect(() => {
    if (isPlaying && isWorkerInitialized && videoRef.current) {
      const loop = (timestamp: number) => {
        detectFrame(timestamp);
        animationRef.current = requestAnimationFrame(loop);
      };

      animationRef.current = requestAnimationFrame(loop);
    } else if (!isPlaying && animationRef.current) {
      cancelAnimationFrame(animationRef.current);
    }

    return () => {
      if (animationRef.current) {
        cancelAnimationFrame(animationRef.current);
      }
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [isPlaying, isWorkerInitialized]);

  return {
    canvasRef,
    sourceType,
  };
}
